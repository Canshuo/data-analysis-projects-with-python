{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight of evidence (WOE) and information value (IV) \n",
    "\n",
    "A Python example is included at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Weight of Evidence (WOE)?\n",
    "The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. \"Bad Customers\" refers to the customers who defaulted on a loan. and \"Good Customers\" refers to the customers who paid back loan.\n",
    "The formula to calculate the weight of evidence for any feature is given by\n",
    "![title](https://editor.analyticsvidhya.com/uploads/30208woe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of the categories/bins of a feature has a large proportion of events compared to the proportion of non-events, we will get a high value of WoE which in turn says that that class of the feature separates the events from non-events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why WoE is often seen in logistic regression?\n",
    "> - First, logistic regression is a parametric method that requires us to calculate a linear equation. This requires that all features are numerical. However, we might have categorical features in our datasets that are either nominal or ordinal. For this regard, WoE values for the various categories of a categorical variable can be used to impute a categorical feature and convert it into a numerical feature;\n",
    "> - Second, the WoE of a feature has a linear relationship with the log odds. This ensures that the requirement of the features having linear relation with the log odds is satisfied.\n",
    "> - Third, if a continuous feature does not have a linear relationship with the log odds, the feature can be binned into groups and a new feature created by replaced each bin with its WoE value can be used instead of the original feature. In other words, WoE transformation helps you to build strict linear relationship with log odds. Otherwise it is not easy to accomplish linear relationship using other transformation methods such as log, square-root etc.\n",
    "\n",
    "### Other benefits of WoE:\n",
    "> - It can treat outliers. Suppose you have a continuous variable such as annual salary and extreme values are more than 500 million dollars. These values would be grouped to a class of (let's say 250-500 million dollars). Later, instead of using the raw values, we would be using WOE scores of each classes.\n",
    "> - It can handle missing values as missing values can be binned separately.\n",
    "> - Since WOE Transformation handles categorical variable so there is no need for dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of WOE\n",
    "\n",
    "Weight of Evidence (WOE) helps to transform a continuous independent variable into a set of groups or bins based on similarity of dependent variable distribution i.e. number of events and non-events. \n",
    "\n",
    "__For continuous independent variables__: First, create bins (categories / groups) for a continuous independent variable and then combine categories with similar WOE values and replace categories with WOE values. Use WOE values rather than input values in your model.\n",
    "\n",
    "__For categorical independent variables__: Combine categories with similar WOE and then create new categories of an independent variable with continuous WOE values. In other words, use WOE values rather than raw categories in your model. The transformed variable will be a continuous variable with WOE values. It is same as any continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to check correct binning with WOE?\n",
    "- Each category (bin) should have at least 5% of the observations.\n",
    "    - Ideally, each bin should contain at least 5% cases. The number of bins determines the amount of smoothing - the fewer bins, the more smoothing. If someone asks you ' \"why not to form 1000 bins?\" The answer is the fewer bins capture important patterns in the data, while leaving out noise. Bins with less than 5% cases might not be a true picture of the data distribution and might lead to model instability.\n",
    "- Each category (bin) should be non-zero for both non-events and events.\n",
    "- The WOE should be distinct for each category. Similar groups should be aggregated.\n",
    "    - why? It is because the categories with similar WOE have almost same proportion of events and non-events. In other words, the behavior of both the categories is same.\n",
    "- The WOE should be monotonic, i.e. either growing or decreasing with the groupings.\n",
    "- Missing values are binned separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Value\n",
    "Having discussed the WoE value, the WoE value tells us the predictive power of each bin of a feature. However, a single value representing the entire feature’s predictive power will be useful in feature selection.\n",
    "\n",
    "The equation for IV is\n",
    "![title](https://editor.analyticsvidhya.com/uploads/66585IV.png)\n",
    "\n",
    "> Note that the term (percentage of events – the percentage of non-events) follows the same sign as WoE hence ensuring that the IV is always a positive number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpretation the IV value\n",
    "<table>\n",
    "<tr>\n",
    "<td>Information Value </td>\n",
    "<td>Variable Predictiveness</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Less than 0.02 </td>\n",
    "<td>Not useful for prediction</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.02 to 0.1 </td>\n",
    "<td>Weak predictive Power</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.1 to 0.3 </td>\n",
    "<td>Medium predictive Power</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.3 to 0.5</td>\n",
    "<td>Strong predictive Power</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.5 </td>\n",
    "<td>Suspicious Predictive Power</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "If the IV statistic is:\n",
    "- Less than 0.02, then the predictor is not useful for modeling (separating the Goods from the Bads)\n",
    "- 0.02 to 0.1, then the predictor has only a weak relationship to the Goods/Bads odds ratio\n",
    "- 0.1 to 0.3, then the predictor has a medium strength relationship to the Goods/Bads odds ratio\n",
    "- 0.3 to 0.5, then the predictor has a strong relationship to the Goods/Bads odds ratio.\n",
    "- 0.5, suspicious relationship (Check once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Points\n",
    "1. Information value increases as bins / groups increases for an independent variable. Be careful when there are more than 20 bins as some bins may have a very few number of events and non-events.\n",
    "2. Information value is not an optimal feature (variable) selection method when you are building a classification model other than binary logistic regression (for eg. random forest or SVM) as conditional log odds (which we predict in a logistic regression model) is highly related to the calculation of weight of evidence. In other words, it's designed mainly for binary logistic regression model. Also think this way - Random forest can detect non-linear relationship very well so selecting variables via Information Value and using them in random forest model might not produce the most accurate and robust predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "the calculation of the WoE and IV are beneficial and help us analyze multiple points as listed below\n",
    "\n",
    "1. WoE helps check the linear relationship of a feature with its dependent feature to be used in the model.\n",
    "\n",
    "2. WoE is a good variable transformation method for both continuous and categorical features.\n",
    "\n",
    "3. WoE is better than on-hot encoding as this method of variable transformation does not increase the complexity of the model.\n",
    "\n",
    "4. IV is a good measure of the predictive power of a feature and it also helps point out the suspicious feature.\n",
    "\n",
    "Though WoE and IV are highly useful, always ensure that it is only used with logistic regression. Unlike other feature selection methods available, the features selected using IV might not be the best feature set for a non-linear model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank\n",
       "0      0  380  3.61     3\n",
       "1      1  660  3.67     3\n",
       "2      1  800  4.00     1\n",
       "3      1  640  3.19     4\n",
       "4      0  520  2.93     4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "mydata = pd.read_csv(\"https://stats.idre.ucla.edu/stat/data/binary.csv\")\n",
    "mydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "\n",
    "    #Empty Dataframe\n",
    "    newDF,woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    #Extract Column Names\n",
    "    cols = data.columns\n",
    "    \n",
    "    #Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n",
    "            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n",
    "            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n",
    "        d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = ['Cutoff', 'N', 'Events']\n",
    "        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n",
    "        d['Non-Events'] = d['N'] - d['Events']\n",
    "        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n",
    "        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])\n",
    "        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n",
    "        d.insert(loc=0, column='Variable', value=ivars)\n",
    "        print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))\n",
    "        temp =pd.DataFrame({\"Variable\" : [ivars], \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n",
    "        newDF=pd.concat([newDF,temp], axis=0)\n",
    "        woeDF=pd.concat([woeDF,d], axis=0)\n",
    "\n",
    "        #Show WOE Table\n",
    "        if show_woe == True:\n",
    "            print(d)\n",
    "    return newDF, woeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information value of gre is 0.312882\n",
      "Information value of gpa is 0.27002\n",
      "Information value of rank is 0.292044\n"
     ]
    }
   ],
   "source": [
    "iv, woe = iv_woe(data = mydata, target = 'admit', bins=10, show_woe = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>IV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gre</td>\n",
       "      <td>0.312882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpa</td>\n",
       "      <td>0.270020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rank</td>\n",
       "      <td>0.292044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable        IV\n",
       "0      gre  0.312882\n",
       "0      gpa  0.270020\n",
       "0     rank  0.292044"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Cutoff</th>\n",
       "      <th>N</th>\n",
       "      <th>Events</th>\n",
       "      <th>% of Events</th>\n",
       "      <th>Non-Events</th>\n",
       "      <th>% of Non-Events</th>\n",
       "      <th>WoE</th>\n",
       "      <th>IV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gre</td>\n",
       "      <td>(219.999, 440.0]</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>42</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>-1.180625</td>\n",
       "      <td>0.125857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gre</td>\n",
       "      <td>(440.0, 500.0]</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>39</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.413370</td>\n",
       "      <td>0.019994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gre</td>\n",
       "      <td>(500.0, 520.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>0.078740</td>\n",
       "      <td>14</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.428812</td>\n",
       "      <td>0.011774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gre</td>\n",
       "      <td>(520.0, 560.0]</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "      <td>0.118110</td>\n",
       "      <td>36</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>-0.110184</td>\n",
       "      <td>0.001516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gre</td>\n",
       "      <td>(560.0, 580.0]</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>23</td>\n",
       "      <td>0.084249</td>\n",
       "      <td>-0.578450</td>\n",
       "      <td>0.021406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gre</td>\n",
       "      <td>(580.0, 620.0]</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>0.165354</td>\n",
       "      <td>32</td>\n",
       "      <td>0.117216</td>\n",
       "      <td>0.344071</td>\n",
       "      <td>0.016563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gre</td>\n",
       "      <td>(620.0, 660.0]</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>0.133858</td>\n",
       "      <td>28</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.266294</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gre</td>\n",
       "      <td>(660.0, 680.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>0.070866</td>\n",
       "      <td>11</td>\n",
       "      <td>0.040293</td>\n",
       "      <td>0.564614</td>\n",
       "      <td>0.017262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gre</td>\n",
       "      <td>(680.0, 740.0]</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>32</td>\n",
       "      <td>0.117216</td>\n",
       "      <td>-0.215545</td>\n",
       "      <td>0.004899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gre</td>\n",
       "      <td>(740.0, 800.0]</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>0.149606</td>\n",
       "      <td>16</td>\n",
       "      <td>0.058608</td>\n",
       "      <td>0.937135</td>\n",
       "      <td>0.085278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(2.259, 2.9]</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>0.062992</td>\n",
       "      <td>35</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>-0.710622</td>\n",
       "      <td>0.046342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(2.9, 3.048]</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>26</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>-0.094917</td>\n",
       "      <td>0.000819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(3.048, 3.17]</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>0.062992</td>\n",
       "      <td>34</td>\n",
       "      <td>0.124542</td>\n",
       "      <td>-0.681634</td>\n",
       "      <td>0.041955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(3.17, 3.31]</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>0.078740</td>\n",
       "      <td>32</td>\n",
       "      <td>0.117216</td>\n",
       "      <td>-0.397866</td>\n",
       "      <td>0.015308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(3.31, 3.395]</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>0.062992</td>\n",
       "      <td>28</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>-0.487478</td>\n",
       "      <td>0.019290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(3.395, 3.494]</td>\n",
       "      <td>40</td>\n",
       "      <td>14</td>\n",
       "      <td>0.110236</td>\n",
       "      <td>26</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.146246</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(3.494, 3.61]</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>0.125984</td>\n",
       "      <td>25</td>\n",
       "      <td>0.091575</td>\n",
       "      <td>0.318998</td>\n",
       "      <td>0.010976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(3.61, 3.752]</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>0.157480</td>\n",
       "      <td>19</td>\n",
       "      <td>0.069597</td>\n",
       "      <td>0.816578</td>\n",
       "      <td>0.071764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(3.752, 3.94]</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>0.102362</td>\n",
       "      <td>29</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>-0.037062</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpa</td>\n",
       "      <td>(3.94, 4.0]</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>0.149606</td>\n",
       "      <td>19</td>\n",
       "      <td>0.069597</td>\n",
       "      <td>0.765285</td>\n",
       "      <td>0.061230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rank</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>33</td>\n",
       "      <td>0.259843</td>\n",
       "      <td>28</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.929588</td>\n",
       "      <td>0.146204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rank</td>\n",
       "      <td>2</td>\n",
       "      <td>151</td>\n",
       "      <td>54</td>\n",
       "      <td>0.425197</td>\n",
       "      <td>97</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.179558</td>\n",
       "      <td>0.012548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rank</td>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>28</td>\n",
       "      <td>0.220472</td>\n",
       "      <td>93</td>\n",
       "      <td>0.340659</td>\n",
       "      <td>-0.435110</td>\n",
       "      <td>0.052295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rank</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>55</td>\n",
       "      <td>0.201465</td>\n",
       "      <td>-0.757142</td>\n",
       "      <td>0.080997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable            Cutoff    N  Events  % of Events  Non-Events  \\\n",
       "0      gre  (219.999, 440.0]   48       6     0.047244          42   \n",
       "1      gre    (440.0, 500.0]   51      12     0.094488          39   \n",
       "2      gre    (500.0, 520.0]   24      10     0.078740          14   \n",
       "3      gre    (520.0, 560.0]   51      15     0.118110          36   \n",
       "4      gre    (560.0, 580.0]   29       6     0.047244          23   \n",
       "5      gre    (580.0, 620.0]   53      21     0.165354          32   \n",
       "6      gre    (620.0, 660.0]   45      17     0.133858          28   \n",
       "7      gre    (660.0, 680.0]   20       9     0.070866          11   \n",
       "8      gre    (680.0, 740.0]   44      12     0.094488          32   \n",
       "9      gre    (740.0, 800.0]   35      19     0.149606          16   \n",
       "0      gpa      (2.259, 2.9]   43       8     0.062992          35   \n",
       "1      gpa      (2.9, 3.048]   37      11     0.086614          26   \n",
       "2      gpa     (3.048, 3.17]   42       8     0.062992          34   \n",
       "3      gpa      (3.17, 3.31]   42      10     0.078740          32   \n",
       "4      gpa     (3.31, 3.395]   36       8     0.062992          28   \n",
       "5      gpa    (3.395, 3.494]   40      14     0.110236          26   \n",
       "6      gpa     (3.494, 3.61]   41      16     0.125984          25   \n",
       "7      gpa     (3.61, 3.752]   39      20     0.157480          19   \n",
       "8      gpa     (3.752, 3.94]   42      13     0.102362          29   \n",
       "9      gpa       (3.94, 4.0]   38      19     0.149606          19   \n",
       "0     rank                 1   61      33     0.259843          28   \n",
       "1     rank                 2  151      54     0.425197          97   \n",
       "2     rank                 3  121      28     0.220472          93   \n",
       "3     rank                 4   67      12     0.094488          55   \n",
       "\n",
       "   % of Non-Events       WoE        IV  \n",
       "0         0.153846 -1.180625  0.125857  \n",
       "1         0.142857 -0.413370  0.019994  \n",
       "2         0.051282  0.428812  0.011774  \n",
       "3         0.131868 -0.110184  0.001516  \n",
       "4         0.084249 -0.578450  0.021406  \n",
       "5         0.117216  0.344071  0.016563  \n",
       "6         0.102564  0.266294  0.008333  \n",
       "7         0.040293  0.564614  0.017262  \n",
       "8         0.117216 -0.215545  0.004899  \n",
       "9         0.058608  0.937135  0.085278  \n",
       "0         0.128205 -0.710622  0.046342  \n",
       "1         0.095238 -0.094917  0.000819  \n",
       "2         0.124542 -0.681634  0.041955  \n",
       "3         0.117216 -0.397866  0.015308  \n",
       "4         0.102564 -0.487478  0.019290  \n",
       "5         0.095238  0.146246  0.002193  \n",
       "6         0.091575  0.318998  0.010976  \n",
       "7         0.069597  0.816578  0.071764  \n",
       "8         0.106227 -0.037062  0.000143  \n",
       "9         0.069597  0.765285  0.061230  \n",
       "0         0.102564  0.929588  0.146204  \n",
       "1         0.355311  0.179558  0.012548  \n",
       "2         0.340659 -0.435110  0.052295  \n",
       "3         0.201465 -0.757142  0.080997  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
